\documentclass[sigplan,review,anonymous]{acmart}
\usepackage{minted}
\usepackage{algorithm2e}
\usepackage{natbib}

\begin{document}

\title{Gouki Scheme: An embedded Scheme implementation for Async Rust}

\author{Matthew Plant}
\affiliation{%
  \institution{OneChronos}
  \city{New York}
  \state{New York}
  \country{USA}
}
\email{maplant@protonmail.com}

\begin{abstract}

  GoukiScheme is the first-of-its-kind JIT compiler for the R6RS dialect of
  Scheme that supports \
  call-with-current-continuation while being embeddable
  within a compiled language with first class support for async/await.

\end{abstract}

\maketitle

\section{Introduction}

Over the last decade Rust has become an increasingly popular choice for systems
programming. Since the introduction of async runtimes to the Rust language,
async Rust programs have also become increasingly popular for writing highly
IO bound applications such as web servers or embedded programming. While Rust
is extremely performant for such applications\cite{embedded}, async Rust
applications tend to be difficult to develop and debug with\footnote{\citet{debugger} claims: the debugging story is not great. In particular, answering the question “why isn’t my program currently doing anything” is very hard.}
. Part of the problem
is long compilation times\cite{slow}; Rust applications must be stopped, rebuilt, and
restarted, extending development time and reducing the ability for developers to
iterate.

Glue code written in a dynamic language has long been a solution for enabling
rapid prototyping and interoperability in a language with long build times. Under
this model, the performance critical code that does not  need to be rebuilt is
written in the slower compiled language, while a dynamic perhaps interpreted
language is embedded into the build to allow for gluing components together
without the need for slow rebuilds. Changes to the glue code can be seen just by
re-loading the application, or perhaps a hot reloading mechanism can bring them
into the application simply by saving to a file. In either case, iteration
speeds are improved dramatically.

Another advantage of embedding a dynamic language in an application is that the
application can provide a Read-Eval-Print-Loop, or REPL. Interactive prompts can
expand the debugging capabilities of a application by allowing for inspection of
the application while it is live. The application can be debugged, inspected, and
orchestrated as a plastic system rather than a rigid daemon that can only be
started, stopped or interacted with in small fixed commands.

Scheme has shown success as a embedded dynamic language, but its use has been
limited to synchronous (but perhaps multithreaded) applications. Scheme
implementations exist for embedding within Rust, but they are limited to running
sync Rust code and cannot be async themselves or they require the overhead of an
interpreter or bytecode virtual machine. Since async is a Rust language feature
that touches all parts of the code base, a new implementation of Scheme
is required that is built with async Rust in mind to take full advantage of the
async runtime and more importantly integrate frictionlessly into the async
application while also maximizing performance with JIT compilation.

GoukiScheme is a Scheme implementation designed to integrate
flawlessly with async Rust; GoukiScheme code can execute asynchronously embedded
in an async Rust application and can in turn execute async Rust functions. Arbitrary Rust
objects can be stored in GoukiScheme variables and passed to GoukiScheme
functions with little modification. This is done without sacrificing potential
performance by architecting GoukiScheme as a tree-walking AST; indeed GoukiScheme
is fully JIT compiled and takes advantage of a CPS based mid-level IR to compile
to LLVM SSA. GoukiScheme does this while providing a completely Safe API.

This integration allows for Scheme programs to be written that take advantage
of the Rust async ecosystem, such as this example of an echo server adapted
from Tokio\cite{tokio}:

\begin{minted}[frame=lines]{scheme}
(define (echo socket)
  (let ((buff (await (read-all socket))))
    (await (write-all socket buff))
    (echo socket)))

(define (listen listener)
  (let-values (((socket _) (await (accept listener))))
    (spawn (lambda () (echo socket)))
    (listen listener)))

(listen (await (bind-tcp "127.0.0.1:8080")))
\end{minted}

In this example, the functions \texttt{read-all}, \texttt{write-all},
\texttt{accept}, and \texttt{bind-tcp} are all Rust functions accessible via
Scheme that call the appropriate tokio functions. \texttt{await} is also
a builtin that checks that its argument is a \texttt{Future}\cite{future} and
awaits it.

\section{Alternatives}

There are several solutions currently available for embedding Scheme in Rust,
one of the most straightforward is to use the C bindings provided by the Guile
implementation, of which there are several bindings available in the Rust
registry, although most of them are fairly rudimentary and incomplete.
Additionally, none of these implementations provide any support for async Rust,
since no such support is available from Guile.

Steel\cite{steel} is a pure Rust interpreter for Scheme that has support for
calling async functions and utilizing async runtimes. Gouki and Steel differ in
two primary ways. The first is in implementation: Steel is implemented via a
bytecode interpreter, while Gouki is JIT compiled. The second is to what degree
async is required by the implementation: Steel allows for an async runtime to
be provided optionally, while Gouki requires an async runtime as that is its
primary use case.

At the time of publication, Gouki Scheme is the only JIT compiled implementation
of a language with support for \texttt{call-with-current-continuation} that can
be embedded in a compiled language with support for async programming.

\section{Memory Management}

In order to properly implement the Scheme programming language, some form of
garbage collection must be implemented by the system. One very popular algorithm
for garbage collection is known as tracing, in which objects that are not
determined to be reachable from so-called ``root'' objects are retained while 
the remaining objects are considered garbage and deallocated\cite{lisptracing}.

The root objects of a Scheme program are local and global variables active on the
current Scheme call-stack, which is easy for our implementation to collect.
However, when values escape into Rust code, the root objects for the entire
program must be expanded to include that of the Rust code as well. Unfortunately,
determining the roots of Rust program is not feasible\cite{rusttracing}.

A common Rust technique for automatic memory management is to use reference
counting, which does not require knowing the roots of a program. However, this
by itself this insufficient for our implementation due to the inability of
reference counted systems to collect cycles\cite{rc}. We can extend this
technique to support all valid Scheme programs by utilizing concurrent cycle
collection\cite{cc}.

The API for garbage collection in GoukiScheme is simple. GoukiScheme provides a
\texttt{Gc} type that implements the same interface as Arc:

\begin{minted}[frame=lines]{rust}
pub struct Gc<T>
where
  T: ?Sized,
{
  // private fields
}
\end{minted}

Fundamentally the \texttt{Gc} type intends to mimic the behavior of a scheme variable.
It can be read from and written to and arbitrarily passed around as a
reference. Scheme variables can be passed to Rust code as a Gc. 

Allocating a new garbage-collected T is done via the \texttt{Gc::new} function, and
creating a new copy of the pointer to that object is done via the \texttt{Clone}
method.

Additionally, the \texttt{Gc} type provides a \texttt{write} and \texttt{read}
methods in order to provide thread-safe mutations and accesses to the allocated
data. Since GoukiScheme can arbitrarily spawn threads that reference shared or
global variables, it is important for all writes and reads to variables to be safe
and atomic. This is achieved by embedded reader-writer lock in the Gc type.

After the \texttt{init\_gc} function is called once, GoukiScheme spawns a task
dedicated to collecting garbage. This task is shut down automatically when the
main function returns. This provides a smooth and intuitive interface to the
memory manager that is also performant. Subsequent calls to \texttt{init\_gc}
are a no-op

\begin{minted}[frame=lines]{rust}
#[tokio::main]
fn main() {
  init_gc();
  let a = Gc::new("hello");
  let b = a.clone();
  {
    *a.write() = "world";
  }
  assert_eq!(b.read(), "world");
}
\end{minted}

Whenever a \texttt{Gc} is cloned or dropped, the change in reference count is
sent to the collector task over an unbounded channel called the mutation buffer.
The collector task receives those mutations in a loop, in what essentially amounts
to the following code:

\begin{minted}[frame=lines]{rust}
fn init_gc() {
  // Spawn the collector task:
  let _ = spawn(async move {
    let mut mutations = Vec::new();
    loop {
       BUFFER.recv_many(
         &mut mutations
       ).await;
       process_mutation(mutation);
       mutations.clear();
    } 
  });
}  
\end{minted}

The collector can be an asynchronous coroutine (in tokio what is called a task)
because its loop is receiving from an asynchronous channel. Since it is a task,
which is a \texttt{Future} (also known as a promise), it is subject to
cancellation\cite{cancellation} when it yields at \texttt{.await} points.
One common cause of cancellation is when the tokio runtime is shutdown,
i.e. when the main function returns. Therefore, no manual intervention is
required by the user to shut down the garbage collection task, as it will
properly shut down at the end of the program's lifetime.

\subsection{Tracing and Finalizing}

One limitation of the cycle collection scheme (and indeed even tracing garbage
collection schemes) and therefore the \texttt{Gc} smart pointer is the
requirement to enumerate every reference from within a single \texttt{Gc} in
order to properly determine cycles. Additionally, properly finalizing a cycle is
non-trivial and needs information about the contents of the type. This limits us
to storing types in a Gc that implement the \texttt{Trace} trait:

\begin{minted}[frame=lines]{rust}
unsafe trait Trace: 'static {
  unsafe fn visit_children(
    &self,
    visitor: unsafe fn(OpaqueGcPtr)
  );

  unsafe fn finalize(&mut self);
}
\end{minted}

Implementing this function is tedious and requires the user to implement
potentially memory unsafe code. However, it is not difficult to implement based
on the syntactic form of the data type being implemented from. For example, here
is the corresponding \texttt{Trace} implementation for a sample struct:

\begin{minted}[frame=lines]{rust}
struct Example {
  a: Gc<Example>,
  b: OtherStruct,
}

unsafe impl Trace for Example {
  unsafe fn visit_children(
    &self,
    visitor: unsafe fn(OpaqueGcPtr)
  ) {
    visitor(self.a.as_opaque());
    self.b.visit_children(visitor);
  }

  unsafe fn finalize(&mut self) {
    self.b.finalize();
  }
}
\end{minted}

The rules for implementing a Trace for a function is simple. First, we must
assume that each field implements \texttt{Trace} or is a \texttt{Gc}. If the
fields do not implement \texttt{Trace} and are not a \texttt{Gc}, then a
compilation error will surface later when we attempt to call the
\texttt{visit\_children} and \texttt{finalize} functions. After we assume this,
we can implement \texttt{Trace} as follows:

\begin{algorithm}
  \SetKwFunction{Visitor}{Visitor}
  \SetKwFunction{VisitChildren}{VisitChildren}
  \SetKwFunction{AsOpaque}{AsOpaque}
  \SetKwFunction{Gc}{Gc}
  \SetKwFunction{typeof}{typeof}
  \caption{visit children}\label{alg:cap}

  \For{$field \in fields$}{
    \lIf{\typeof{$field$} $=$ \Gc}{ \Visitor{\AsOpaque{$field$}} }
    \lElse{ \VisitChildren{$field$} }
  }
\end{algorithm}

\begin{algorithm}
  \SetKwFunction{Finalize}{Finalize}
  \SetKwFunction{Gc}{Gc}
  \SetKwFunction{typeof}{typeof}
  \caption{finalize}\label{alg:cap}

  \For{$field \in fields$}{
    \If{\typeof{$field$} $\ne$ \Gc}{
      \Finalize{$field$}
    }
  }
\end{algorithm}

Since these algorithms can be clearly implemented from the syntactic form of the
data type (or rather it nearly can; we can assume the type of a field is equal to
a \texttt{Gc} if it is spelled \texttt{Gc} - while this is not true in some
pathological cases, such as when a local \texttt{Gc} type is defined, it is a
good enough heuristic for nearly all cases), they are a prime candidates to be
implemented with a procedural macro, or more specifically a derive
macro\cite{derive}:

\begin{minted}[frame=lines]{rust}
#[derive(Trace)]
struct NamedCell {
  name: &'static str,
  cell: Gc<Value>,
}

let foo = Gc::new(NamedCell { .. });
\end{minted}

While this covers a large number of cases, some types can never implement
\texttt{Trace}. One such example is types defined in external crates that do not
already implement \texttt{Trace}; such types cannot implement \texttt{Trace} due
to the orphan rule\cite{orphan}. Storing these types requires an extra level of
indirection through Rust's built-in reference counted type, the
\texttt{Arc}\cite{arc}. Extra care must be taken when using this escape hatch as
previously mentioned these types do not provide any way to collect cyclical
references.

\subsection{Enabling Back-Pressure from the Garbage Collector}

Because we are running the garbage collector concurrently and are sending
mutations to the collector over an unbounded channel, the allocating tasks
receive no back pressure from the collector task. As a result should the pace
of new allocations outpace the collectors ability to free garbage, the process
would exhibit what appears to be a memory leak. While rare, it can still occur
and thus we allow for a way to convert the unbounded channel into backpressure:

\begin{minted}[frame=lines]{rust}
async fn yield_until_gc_cleared() {
  while PENDING_MUTATIONS > MAX_ALLOWED {
    tokio::task::yield_now().await
  }
}
\end{minted}

This function is automatically called in the evaluation trampoline to prevent
runaway memory allocations. This means that while the collection scheme is
always concurrent, it is only \textit{parallel} for some amount of pending
mutations.

\section{Values}

The \texttt{Value} type represents a Scheme value. It has the dual responsibility
of store any possible Scheme value while also allowing for the storage of any
Rust value. Additionally, if a Scheme value has a reasonable native equivalent
in Rust (which almost all of them do), conversion should be convenient and
efficient. The \texttt{Value} type should also be convenient to pass to and use in our
JIT compiled functions:

\begin{minted}[frame=lines]{rust}
#[repr(transparent)]
pub struct Value(u64);
\end{minted}

We use a tagged pointer\cite{tags} scheme to achieve these constraints with
minimal overhead. A rather large tag size of four bits allowing for 16 tags in
order to represent the most common Scheme values without an extra level of
indirection.

\begin{minted}[frame=lines]{rust}
pub enum ValueType {
    Null = 0,
    Boolean = 1,
    Character = 2,
    Number = 3,
    String = 4,
    Symbol = 5,
    Vector = 6,
    ByteVector = 7,
    Syntax = 8,
    Closure = 9,
    Record = 10,
    RecordType = 11,
    Pair = 12,
    SchemeCompatible = 13,
    HashMap = 14,
}
\end{minted}

As a small optimization, an extra "Undefined" value, the result of reading a
variable that has not been defined, is a Pair with a null pointer, or a value
of $12$.

This large number of tags allows us to map all of the primitive values in the
serde data model\cite{serde} into a single \texttt{Value} without an extra level of
indirection. As a result, any Rust type that implements Serialize can
automatically be converted into a \texttt{Value}.

We also utilize Rust's dynamic dispatch to allow for the storing of any type that
implements the \texttt{SchemeCompatible} and \texttt{Trace} traits. Using this
trait we can create new Scheme values that present themselves as sealed records.

\begin{minted}[frame=lines]{rust}
trait SchemeCompatible: Any {
  fn record_type(&self) -> Arc<RecordType>;

  fn eqv(&self, rhs: &Value) -> bool;
}
\end{minted}

Objects that implement this trait can be stored as a \texttt{Gc<dyn SchemeCompatible>},
which can then be converted into a \texttt{Value}. Values, in turn, can be converted
back to a \texttt{Gc<dyn SchemeCompatible>}, which can then be downcast to a concrete
type.

\section{Evaluation}

Evaluation of Scheme code in Rust is achieved through two primary objects: the
\texttt{Closure} and \texttt{Application} struct.

As the name implies, the \texttt{Closure} struct represents a Scheme closure,
which includes a captured environment and some code:

\begin{minted}[frame=lines]{rust}
struct Closure {
  env:     Box<[Gc<Value>]>,
  func:    FuncPtr,
  runtime: Gc<Runtime>,
  // some fields omitted
}
\end{minted}

The env field points to an array of variables that can be accessed from the
function. It includes all non-local variables that are captured as part of
reifying a function definition into a closure.

The function pointer type \texttt{FuncPtr} is the heart of the bridge between
asynchronous Rust and Scheme code. It is sum type of three different pointer
types:

\begin{minted}[frame=lines]{rust}
pub enum FuncPtr {
  Continuation(ContinuationPtr),
  Closure(ClosurePtr),
  Bridge(BridgePtr),
}
\end{minted}

The \texttt{ContinuationPtr} and \texttt{ClosurePtr} types are JIT compiled
Scheme functions and the \texttt{BridgePtr} type is a Rust function that has
been cast to a function pointer. Therefore all Scheme code is treated
synchronously but can switch over to async Rust whenever it needs to await the
result of a future. This is achieved by a trampoline; each call to a function
returns an application to another function:

\begin{minted}[frame=lines]{rust}
pub struct Application {
  op:   Option<Gc<Closure>>,
  args: Vec<Value>,
  // some fields omitted 
}
\end{minted}

However, in the case of Rust bridge functions - which can be potentially async -
the function returns the \textit{future} of an application. That means the body of
trampoline loop is async:

\begin{minted}[frame=lines]{rust}
impl Application {
  pub async fn(mut self) ->
    Result<Vec<Value>, Exception>
  {
    while let Application {
       op: Some(op),
       args,
    } = self
    {
      self = op.apply(&args).await;
    }
  }
}
\end{minted}

This allows for the JIT compiled \texttt{ContinuationPtr} and
\texttt{ClosurePtr}s and for the Rust bridge functions to seamlessly interact
with each other. The synchronous JIT compiled function pass their continuation
to the async Rust code to evaluate the async result after it is awaited in the
CPS trampoline.

\section{Registering Bridge Functions}

Using the inventory\cite{inventory} crate we can create a global registry of Rust
functions which can be imported into Scheme code. Functions with the signature of
\texttt{BridgePtr} can be registered manually, but a procedural macro is provided
to convert functions of many different signatures:

\begin{minted}[frame=lines]{rust}
#[bridge(
  name = "read-file-to-string",
  lib = "(tokio)"
)]
async fn read_file(file: &Value) ->
  Result<Vec<Value>, Condition>
{
  let file = file.to_string();
  let contents =
    read_to_string(&file)
    .await
    .map_err(|_| Condition::Error)?;
  Ok(vec![Value::from(contents)])
}
\end{minted}

Every function that registers with this inventory will be available automatically
defined in their respective library in any \texttt{Registry} object created,
GoukiScheme's notion of a package registry.

\section{JIT Compilation}

After the macro expansion phase, Scheme code is converted into a simple, reduced
CPS mid level IR using the algorithm described in Compiling with
Continuations\cite{cwc}. After optimization the CPS IR is converted to
LLVM SSA\cite{llvm} for JIT compilation. LLVM was chosen due to its relative
familiarity and large set of supported platforms, but the relative simplicity of
the CPS mid level IR does not rule out the adding of additional backends such
as Cranelift\cite{cranelift} which may have different performance
characteristics and trade-offs.

Interacting with the LLVM JIT compiler to produce an object that can easily be
interacted with in Rust (i.e. a \texttt{Closure} object described earlier)
requires some care. When a \texttt{Closure} goes out of scope, we need a way to
free the memory allocated for its function. LLVM's JIT compiler requires that
we only have one handle to this memory per thread, which we call the
\texttt{Context}\cite{context}. In order to provide as safe interface that
satisfies this guarantee, we create a handle to a separate thread that fully owns
the JIT executor, called the \texttt{Runtime}:

\begin{minted}[frame=lines]{rust}
struct Runtime {
  comp_tasks_tx: Sender<CompilationTask>
}
\end{minted}

Creating a new \texttt{Runtime} spawns a new thread which handles all of our
compilation tasks. When the Runtime goes out of scope, its sender is dropped
and the compilation task exits, freeing all of the JIT compiled functions.

Once we have a context for which we can JIT compile functions, we need to send
those functions back to the caller, via function pointers. In Rust function
pointers are of the static lifetime, which is to say they have the lifetime of
the entire program. This is obviously incorrect in this instance as the lifetime
of \texttt{Runtime} is not guaranteed to last the entire program - a user could
create a \texttt{Runtime}, create a \texttt{Closure} from that runtime and
summarily drop the given \texttt{Runtime} while continuing to use the
\texttt{Closure}. Therefore, we need to add a pointer from the Closure back to
the Runtime from which it was created:

\begin{minted}[frame=lines]{rust}
struct Closure {
  // ...
  runtime: Gc<Runtime>,
}
\end{minted}

This is ensures that if the \texttt{Runtime} is dropped before the
\texttt{Closure}, there will still be a live reference preventing the function
pointers from being deallocated.

\section{Putting It All Together}

Now that we have all of the requisite pieces, we can construct a Rust program
that compiles and executes async Scheme code at runtime:

\begin{minted}[frame=lines]{rust}
let rt = Runtime::new();
let registry = Registry::new(&runtime).await;
let env = Environment::from(
  registry.import("(base)")
);
let value = rt.eval(
  &env,
  // Sleep for 10 seconds and return a string
  "(begin (await (sleep 10)) \"hello world\")"
).await.unwrap();
println!("{value}");
\end{minted}

\section{Conclusion}

GoukiScheme provides a purely safe, easy-to-use interface for interacting with
async Scheme code from within async Rust. It does this without sacrificing
performance by implementing a novel JIT compilation and evaluation scheme
that takes advantage of an async trampoline and eschewing typical tracing
garbage collectors for a concurrent cycle collection scheme. 

\bibliographystyle{ACM-Reference-Format}
\bibliography{GoukiScheme}

\end{document}
\endinput
